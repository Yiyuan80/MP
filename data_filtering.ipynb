{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_filtering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWuL4yd9ZMXjni14/gJkzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yiyuan80/MP/blob/main/data_filtering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1whXh8EUX3SX",
        "outputId": "bfb06da7-5522-4359-894e-001b59a1c565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import MultipleLocator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, MaxPool3D, Dropout, AveragePooling3D, GlobalAveragePooling2D, GlobalMaxPool2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow import keras\n",
        "import time\n",
        "import itertools\n",
        "from itertools import combinations"
      ],
      "metadata": {
        "id": "yei0t0SlX-fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_name(filedir):\n",
        "  \"\"\"Filter names with several conditions and return a name list that satisfy the conditions.\"\"\"\n",
        "  filenames = [f for f in os.listdir(filedir) if f[-4:] == '.csv']\n",
        "\n",
        "  id = []\n",
        "  all_wake = []\n",
        "  all_act = []\n",
        "  max_length = 0\n",
        "\n",
        "  for file in filenames:\n",
        "\n",
        "    print('Parsing', file)\n",
        "    df = pd.read_csv(os.path.join(filedir, file))\n",
        "    last_day = df['daybymidnight'].unique()[-1]\n",
        "    # impute missingness less than 30 and exclude days with >30 missing\n",
        "    missing = df.loc[df['interval']=='EXCLUDED']\n",
        "    missing_count = missing.groupby(['daybymidnight']).count()\n",
        "    missing_exclude = missing_count.loc[missing_count['interval']>30]\n",
        "    missing_impute = missing_count.loc[missing_count['interval']<=30]\n",
        "    missing_ex_index = list(missing_exclude.index)\n",
        "    missing_ex_index.append(1)\n",
        "    missing_ex_index.append(last_day)\n",
        "    missing_im_index = list(missing_impute.index)\n",
        "    df_excluded = df[~df['daybymidnight'].isin(missing_ex_index)] # exclude days with larger than 30 excluded\n",
        "    # df_impute = df_excluded[df_excluded['daybymidnight'].isin(missing_im_index)] # find days left need to be imputed\n",
        "    df_imputed = df_excluded.fillna(method='ffill') # conduct forward fill\n",
        "\n",
        "    # filter data with at least 5-consecutive days\n",
        "    # days_imputed = df_imputed['daybymidnight'].unique()\n",
        "    days = df_excluded['daybymidnight'].unique()\n",
        "\n",
        "    if detect_consecutive(days,5):\n",
        "      id.append(os.path.join(filedir, file))\n",
        "      start_epoch = time_to_epoch(df_imputed['linetime'].values[0])\n",
        "      end_epoch = time_to_epoch(df_imputed['linetime'].values[-1])\n",
        "\n",
        "      wake = df_imputed['wake'].values\n",
        "      act = df_imputed['activity'].values\n",
        "\n",
        "      start_pad = np.array([np.nan] * start_epoch)\n",
        "\n",
        "      wake = np.concatenate([start_pad, wake])\n",
        "      act = np.concatenate([start_pad, act])\n",
        "\n",
        "      max_length = max(max_length, len(wake))\n",
        "\n",
        "      all_wake.append(wake.astype('bool'))\n",
        "      all_act.append(act)\n",
        "\n",
        "  # save id to txt  \n",
        "  with open(\"/content/drive/MyDrive/mesa/filtered_id.txt\", \"w\") as file:\n",
        "    for row in id:\n",
        "      s = \"\".join(map(str, row))\n",
        "      file.write(s+'\\n')\n",
        "\n",
        "  if (max_length % 2880) > 0:\n",
        "    max_length += 2880 - (max_length % 2880)\n",
        "\n",
        "  all_wake = np.stack([\n",
        "\t\tnanpad(w, max_length).reshape(-1, 2880).astype('bool')\n",
        "\t\tfor w in all_wake])\n",
        "  all_act = np.stack([\n",
        "\t\tnanpad(w, max_length).reshape(-1, 2880).astype('float32')\n",
        "\t\tfor w in all_act])\n",
        "\n",
        "  print('Creating .npy files for wake and physical activity in the current directory.')\n",
        "\n",
        "  np.save('wake.npy', all_wake)\n",
        "  np.save('activity.npy', all_act)\n",
        "\n",
        "\n",
        "def nanpad(arr, l):\n",
        "\tpad_length = l - len(arr)\n",
        "\treturn np.concatenate([arr, np.array([np.nan] * pad_length)])\n",
        "\n",
        "\n",
        "def time_to_epoch(timestring):\n",
        "\thour, minute, second = [int(x) for x in timestring.split(':')]\n",
        "\treturn int(hour * 120 + minute * 2 + second / 30)"
      ],
      "metadata": {
        "id": "2oQ-SPugYEuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_consecutive(days, n):\n",
        "  \"\"\"check whether input data contains at least n consecutive days.\"\"\"\n",
        "  diff_days = list(np.diff(days)==1)\n",
        "  runs = [len(list(g)) for _,g in itertools.groupby(diff_days)]\n",
        "  if len(runs)==0 or max(runs) < n-1:\n",
        "    return False\n",
        "  elif max(runs) >= n-1: # test difference between days contain consecutive n-1 \n",
        "    return True"
      ],
      "metadata": {
        "id": "na-9U3uPYLeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_consecutive(days):\n",
        "  \"\"\"Select 5 consecutive days from a participant.\"\"\"\n",
        "  for i in days:\n",
        "    conse = [i+1, i+2, i+3, i+4]\n",
        "    if all(item in days for item in conse):\n",
        "      return i"
      ],
      "metadata": {
        "id": "ITHnujG1YNMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_combination(wake):\n",
        "  \"\"\"generate all postive combinations.\"\"\"\n",
        "  all_pos = []\n",
        "  mtsy = []\n",
        "  for i in range(len(wake[:,1,1])):\n",
        "    day_epochs = tf.reduce_sum(wake[i,:,:], axis=1)\n",
        "    days = np.where(day_epochs!=2880)[0]\n",
        "    days_comb = list(combinations(days[0:-2],2))\n",
        "    for comb in days_comb:\n",
        "      all_pos.append(np.stack([wake[i, comb[0]:comb[0]+3,:],wake[i, comb[1]:comb[1]+3,:]]))\n",
        "\n",
        "  pos = np.stack(all_pos)\n",
        "  np.save('/content/drive/MyDrive/mesa/saved_data/positive.npy', pos)"
      ],
      "metadata": {
        "id": "sZUWq6YuZNnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "\t# filedir = input('Enter directory with MESA actigraphy files:')\n",
        "\t# filenames = [f for f in os.listdir(filedir) if f[-4:] == '.csv']\n",
        "\tfilenames = id_list\n",
        "\n",
        "\tall_wake = []\n",
        "\tall_act = []\n",
        "\n",
        "\tmax_length = 0\n",
        "\n",
        "\tfor file in filenames:\n",
        "\n",
        "\t\tprint('Parsing', file)\n",
        "\t\t\n",
        "\t\t# df = pd.read_csv(os.path.join(filedir, file))\n",
        "\t\tdf = pd.read_csv(file)\n",
        "\t\t\n",
        "\t\tstart_epoch = time_to_epoch(df['linetime'].values[0])\n",
        "\t\tend_epoch = time_to_epoch(df['linetime'].values[-1])\n",
        "\n",
        "\t\twake = df['wake'].values\n",
        "\t\tact = df['activity'].values\n",
        "\n",
        "\t\tstart_pad = np.array([np.nan] * start_epoch)\n",
        "\n",
        "\t\twake = np.concatenate([start_pad, wake])\n",
        "\t\tact = np.concatenate([start_pad, act])\n",
        "\n",
        "\t\tmax_length = max(max_length, len(wake))\n",
        "\n",
        "\t\tall_wake.append(wake.astype('bool'))\n",
        "\t\tall_act.append(act)\n",
        "\n",
        "\tif (max_length % 2880) > 0:\n",
        "\t\tmax_length += 2880 - (max_length % 2880)\n",
        "\n",
        "\tall_wake = np.stack([\n",
        "\t\tnanpad(w, max_length).reshape(-1, 2880).astype('bool')\n",
        "\t\tfor w in all_wake])\n",
        "\tall_act = np.stack([\n",
        "\t\tnanpad(w, max_length).reshape(-1, 2880).astype('float32')\n",
        "\t\tfor w in all_act])\n",
        "\n",
        "\tprint('Creating .npy files for wake and physical activity in the current directory.')\n",
        "\n",
        "\tnp.save('wake.npy', all_wake)\n",
        "\tnp.save('activity.npy', all_act)\n",
        "\n",
        "\n",
        "def nanpad(arr, l):\n",
        "\tpad_length = l - len(arr)\n",
        "\treturn np.concatenate([arr, np.array([np.nan] * pad_length)])\n",
        "\n",
        "\n",
        "def time_to_epoch(timestring):\n",
        "\thour, minute, second = [int(x) for x in timestring.split(':')]\n",
        "\treturn int(hour * 120 + minute * 2 + second / 30)\n",
        "\n"
      ],
      "metadata": {
        "id": "VmohvBvAYQyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}